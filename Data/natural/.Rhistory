title = "the number of vacant housing units in 2016",
subtitle = "",
caption = "Compared with other places in Philadelphia, the percentage
of vacant housing units in Mt Airy is relatively low in 2016.")
telco = read.csv('telco_data.csv')
bank = read.csv('telco_data.csv')
bank = read.csv('bank_churn.csv')
bank = read.csv('telco_data.csv')
bank = read.csv('telco.csv')
bank = read.csv('D:\Upenn\研二上\MKTG_7120\assignment3\telco.csv')
bank = read.csv('D:/Upenn/研二上/MKTG_7120/assignment3/telco.csv')
telco = read.csv('D:/Upenn/研二上/MKTG_7120/assignment3/telco.csv')
View(telco)
View(telco)
telco  = telco[,-1]
head(telco)
m = glm(Churn ~ tenure+PhoneService+InternetService + Contract + MonthlyCharges + PaperlessBilling,
family = "binomial", data = telco)
summary(m)
summary(m)
telco = read.csv('D:/Upenn/研二上/MKTG_7120/assignment3/telco.csv')
telco  = telco[,-1]
head(telco)
m = glm(Churn ~ tenure+PhoneService+InternetService + Contract + MonthlyCharges + PaperlessBilling,
family = "binomial", data = telco)
summary(m)
new = list(tenure = 4, PhoneService = 'Yes', InternetServic = 'Fiber optic',
Contract = 'Month-to-month', MonthlyCharges = 60, PaperlessBilling = 'Yes')
# Then we call predict(), making sure to set type = "response"
predict(m, newdata = new, type = "response")
new = list(tenure = 4, PhoneService = 'Yes', InternetService = 'Fiber optic',
Contract = 'Month-to-month', MonthlyCharges = 60, PaperlessBilling = 'Yes')
# Then we call predict(), making sure to set type = "response"
predict(m, newdata = new, type = "response")
# Then we call predict(), making sure to set type = "response"
predict(m, newdata = new, type = "response")
# if we do not include type = "response"; we get the log-odds instead of probability
predict(m, newdata = new)
View(telco)
m = glm(Churn ~ tenure + PhoneService+InternetService + Contract + MonthlyCharges + PaperlessBilling,
family = "binomial", data = telco)
summary(m)
knitr::opts_chunk$set(message = FALSE, warning = FALSE , results='hide')
knitr::opts_chunk$set(echo = TRUE,  message = FALSE, warning = FALSE)
library(tidyverse)
library(sf)
library(RSocrata)
library(viridis)
library(spatstat)
library(raster)
library(spdep)
library(FNN)
library(grid)
library(gridExtra)
library(knitr)
library(kableExtra)
library(tidycensus)
library(classInt)   # for KDE and ML risk class intervals
library(mapview)
# functions
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")
soil <- st_read('https://raw.githubusercontent.com/JarredERandall/Musa_801_Watauga/main/Data/natural/soil_type.geojson')
soil <- st_transform(soil, st_crs(parcel))
line = st_read('https://raw.githubusercontent.com/JarredERandall/Musa_801_Watauga/main/Data/natural/line.geojson')
parcel = st_read('https://raw.githubusercontent.com/JarredERandall/Musa_801_Watauga/main/Data/natural/parcel.geojson')
soil <- st_read('https://raw.githubusercontent.com/JarredERandall/Musa_801_Watauga/main/Data/natural/soil_type.geojson')
soil <- st_transform(soil, st_crs(parcel))
color_palette2 <- c("#e2a334","#f1ec99ff", "#9ad7d2ff", "#3e7067ff", "#cfd9e0ff")
plot(parcel)
joined_data <- st_join(parcel, soil, join = st_intersects)
View(joined_data)
View(parcel)
library(dplyr)
# Assuming your merged dataset is named 'joined_data'
# Group by FID and Category, then calculate the count of each group
category_counts <- joined_data %>%
filter(Category %in% c('A', 'B', 'B/C', 'C','Unknown')) %>%
group_by(FID, Category) %>%
summarise(count = n()) %>%
ungroup()
# Group by FID and Category, then calculate the count of each group
category_counts <- joined_data %>%
filter(Category %in% c('A', 'B', 'B/C', 'C','Unknown')) %>%
group_by(FID.x, Category) %>%
summarise(count = n()) %>%
ungroup()
# Group by FID and Category, then calculate the count of each group
category_counts <- joined_data %>%
filter(Category %in% c('A', 'B', 'B/C', 'C','Unknown')) %>%
group_by(FID.x, Category) %>%
summarise(count = n()) %>%
ungroup()
# Group by FID and Category, then calculate the count of each group
category_counts <- joined_data %>%
filter(Category %in% c('A', 'B', 'B/C', 'C','Unknown')) %>%
group_by(FID.x, Category) %>%
summarise(count = n()) %>%
ungroup()
# Calculate the total count of each FID
total_counts <- category_counts %>%
group_by(FID.x) %>%
summarise(total_count = sum(count))
# Merge the counts with the total counts to calculate the percentage
category_percentage <- left_join(category_counts, total_counts, by = "FID.x") %>%
mutate(percentage = (count / total_count) * 100)
View(category_counts)
View(total_counts)
```{r}
# Merge the counts with the total counts to calculate the percentage
category_percentage <- st_join(category_counts, total_counts, by = "FID.x") %>%
mutate(percentage = (count / total_count) * 100)
View(category_percentage)
View(joined_data)
setwd("D:/Upenn/2024 Spring/MUSA Practicum/Musa_801_Watauga/Data/natural")
st_write(joined_data, 'joinedSoil.geojson')
View(category_counts)
View(category_counts)
View(category_counts)
View(category_percentage)
# Merge the counts with the total counts to calculate the percentage
category_percentage <- st_join(category_counts, total_counts, by = "FID.x")
View(category_percentage)
total_counts_geometry <- select(total_counts, -geometry)
totalcounts <- st_drop_geometry(total_counts)
categorycounts_geometry <- st_drop_geometry(category_counts)
categorycounts <- st_drop_geometry(category_counts)
totalcounts <- st_drop_geometry(total_counts)
categorycounts <- st_drop_geometry(category_counts)
# Merge the counts with the total counts to calculate the percentage
category_percentage <- merge(totalcounts, categorycounts, by = "FID.x")
View(category_percentage)
# Assuming 'merged_data' is your merged data frame with columns FID.x and Category,
# and each row represents the count of a particular soil type within a parcel.
# Calculate the total count for each parcel
total_counts <- aggregate(Count ~ FID.x, data = merged_data, FUN = sum)
# Calculate the percentage of each soil type in each parcel
category_percentage$Percentage <- (category_percentage$count / category_percentage$total_count) * 100
# Display the resulting data frame
print(category_percentage)
# Assuming 'category_percentage' is your merged data frame with columns 'FID.x', 'total count', 'Category', and 'count'
# Pivot the data frame so that each unique soil type becomes a separate column
pivoted_data <- reshape2::dcast(category_percentage, FID.x + total_count ~ Category, value.var = "count", fill = 0)
# Calculate the percentage of each soil type in each parcel
pivoted_data <- as.data.frame(apply(pivoted_data[, -c(1, 2)], 1, function(row) (row / sum(row)) * 100))
# Rename columns
colnames(pivoted_data) <- paste0(colnames(pivoted_data), "_percentage")
# Merge back the 'FID.x' and 'total count' columns
pivoted_data <- cbind(pivoted_data, category_percentage[, c("FID.x", "total_count")])
library(dplyr)
# Assuming 'category_percentage' is your merged data frame with columns 'FID.x', 'total count', 'Category', and 'count'
# Calculate the percentage of each soil type in each parcel
percentage_data <- category_percentage %>%
group_by(FID.x) %>%
mutate(Percentage = (count / total_count) * 100) %>%
select(FID.x, Category, Percentage) %>%
spread(Category, Percentage, fill = 0)
path_1 <- "D:/UPenn/24 Spring/MUSA Practicum/Musa_801_Watauga/Data/slope_raster/"
slope <- raster(paste0(path_1, "slope.tif"))
path_1 <- "D:/UPenn/2024 Spring/MUSA Practicum/Musa_801_Watauga/Data/slope_raster/"
slope <- raster(paste0(path_1, "slope.tif"))
plot(slope, axes = FALSE, box = FALSE)
# Save the plot to a PNG file with high resolution
suppressMessages({dev.copy(png, filename = "slope_plot.png")
dev.off()})
rec_slope <- raster(paste0(path_1, "rec_slope25.tif"))
plot(rec_slope, axes = FALSE, box = FALSE)
# Save the plot to a PNG file with high resolution
suppressMessages({dev.copy(png, filename = "rec_slope_plot.png")
dev.off()})
plot(slope, axes = FALSE, box = FALSE)
slope_values <- extract(slope_raster, parcel, fun = mean, na.rm = TRUE)
library(raster)
library(sf)
library(raster)
slope_values <- extract(slope_raster, parcel, fun = mean, na.rm = TRUE)
slope_values <- extract(slope, parcel, fun = mean, na.rm = TRUE)
View(slope_values)
View(slope)
View(slope_values)
len(slope_values)
length(slope_values)
length(parcel)
length(parcel)
length(slope_values)
# Load required libraries
# Rasterize the parcel sf object
parcel_raster <- rasterize(parcel, slope)
# Calculate zonal statistics for each parcel
slope_stats <- raster::zonal(slope_raster, parcel_raster, stat = c("max", "min", "mean", "sd"))
# Load required libraries
# Rasterize the parcel sf object
parcel_raster <- rasterize(parcel, slope)
# Calculate zonal statistics for each parcel
slope_stats <- raster::zonal(slope, parcel_raster, stat = c("max", "min", "mean", "sd"))
multi_stats <- function(x) c(max(x, na.rm = TRUE), min(x, na.rm = TRUE), mean(x, na.rm = TRUE), sd(x, na.rm = TRUE))
slope_stats <- raster::zonal(slope, parcel_raster, stat = c("max", "min", "mean", "sd"))
# Calculate zonal statistics for each parcel
slope_stats <- raster::zonal(slope, parcel_raster, fun = c("max", "min", "mean", "sd"))
max_slope_values <- extract(slope, parcel, fun = max, na.rm = TRUE)
max_slope_values <- extract(slope, parcel, fun = max, na.rm = TRUE)
min_slope_values <- extract(slope, parcel, fun = min, na.rm = TRUE)
View(max_slope_values)
View(min_slope_values)
View(line)
View(line)
View(slope_values)
final_parcel <- parcel
knitr::opts_chunk$set(message = FALSE, warning = FALSE , results='hide')
knitr::opts_chunk$set(echo = TRUE,  message = FALSE, warning = FALSE)
library(tidyverse)
library(sf)
library(RSocrata)
library(viridis)
library(spatstat)
library(raster)
library(spdep)
library(FNN)
library(grid)
library(gridExtra)
library(knitr)
library(kableExtra)
library(tidycensus)
library(classInt)   # for KDE and ML risk class intervals
library(mapview)
# functions
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")
landslide = st_read("https://raw.githubusercontent.com/JarredERandall/Musa_801_Watauga/main/Data/natural/landslide.geojson")
boundary = st_read('https://raw.githubusercontent.com/JarredERandall/Musa_801_Watauga/main/Data/natural/Boundary.geojson')
watersupply = st_read('https://raw.githubusercontent.com/JarredERandall/Musa_801_Watauga/main/Data/natural/water_supply.geojson')
line = st_read('https://raw.githubusercontent.com/JarredERandall/Musa_801_Watauga/main/Data/natural/line.geojson')
parcel = st_read('https://raw.githubusercontent.com/JarredERandall/Musa_801_Watauga/main/Data/natural/parcel.geojson')
View(line)
line <- line %>%
rename(Dist_Road = `Road_Dist`)
line2 <- st_transform(line, st_crs(final))
# Calculate the centroid of each parcel
parcel_centroids <- st_centroid(parcel)
distance = parcel
# Add a column named 'centroid_geometry' to the parcel dataset
distance$centroid_geometry <- parcel_centroids
View(line)
final_parcel <- parcel
landslide_geometry <- st_geometry(landslide)
# Convert the geometry to a data frame
landslide_df <- as.data.frame(landslide_geometry)
st_c <- st_coordinates
st_coid <- st_centroid
test1<-st_c(st_coid(parcel))
test2<- st_c(st_coid(landslide))
test3 <- st_c(st_coid(watersupply))
final <- parcel %>%
mutate(
landslide.nn =
nn_function(test1, test2,k=2),
watershed.nn =
nn_function(test1, test3, k = 1))
View(final)
line <- line %>%
rename(Dist_Road = `Road_Dist`)
View(line)
line2 <- st_transform(line, st_crs(final))
combined_df <- cbind(fginal, line2)
line2 <- st_transform(line, st_crs(final))
combined_df <- cbind(final, line2)
View(combined_df)
slope_all <- cbine(slope_values, max_slope_values, min_slope_values)
slope_all <- cbind(slope_values, max_slope_values, min_slope_values)
View(slope_all)
final_parcel <- cbind(combined_df, slope_all)
View(final_parcel)
slope_all <- cbind(slope_values, max_slope_values, min_slope_values)
slope_all <- slope_all %>%
rename(V1 = 'Slope_Ave',
V2 = 'Slope_Max',
V3 = 'Slope_Min')
View(slope_all)
slope_all <- cbind(slope_values, max_slope_values, min_slope_values)
slope_all <- as.data.frame(slope_all)
slope_all <- slope_all %>%
rename(V1 = 'Slope_Ave',
V2 = 'Slope_Max',
V3 = 'Slope_Min')
View(slope_all)
slope_all <- slope_all %>%
rename(V1 = 'Slope_Ave',
V2 = 'Slope_Max',
V3 = 'Slope_Min')
slope_all <- cbind(slope_values, max_slope_values, min_slope_values)
slope_all <- as.data.frame(slope_all)
slope_all <- slope_all %>%
rename(Slope_Ave = V1,
Slope_Max = V2,
Slope_Min = V3)
final_parcel <- cbind(combined_df, slope_all)
View(final_parcel)
setwd("D:/Upenn/2024 Spring/MUSA Practicum/Musa_801_Watauga/Data/natural")
View(final_parcel)
st_write(final_parcel, "natural.geojson")
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, results='hide', fig.keep='all')
soil <- st_read('https://raw.githubusercontent.com/JarredERandall/Musa_801_Watauga/main/Data/natural/soil_type.geojson')
soil <- st_transform(soil, st_crs(parcel))
setwd("C:/Users/Caijingyi/Desktop")
color_palette2 <- c("#e2a334","#f1ec99ff", "#9ad7d2ff", "#3e7067ff", "#cfd9e0ff")
st_write(soil, "soil.shp")
soil_percentage = read.csv("D:/Upenn/2024 Spring/MUSA Practicum/data.soil_percentage.csv")
soil_percentage = read.csv("D:/Upenn/2024 Spring/MUSA Practicum/data.soil_percentage.csv")
soil_percentage = read.csv("D:/Upenn/2024 Spring/MUSA Practicum/data/soil_percentage.csv")
View(soil_percentage)
merged_data <- merge(soil_percentage, final_parcel, by.x = "Join_ID", by.y = "FID", all.x = TRUE)
setwd("D:/Upenn/2024 Spring/MUSA Practicum/Musa_801_Watauga/Data/natural")
st_write(merged_data, "natural.geojson")
View(merged_data)
merged_data <- (mergefinal_parcel, soil_percentage, by.x = "FID", by.y = "Join_ID", all.x = TRUE)
merged_data <- merge(final_parcel, soil_percentage, by.x = "FID", by.y = "Join_ID", all.x = TRUE)
View(merged_data)
# Aggregate the data by Join_ID
aggregated_data <- aggregate(. ~ Join_ID, data = soil_percentage, FUN = sum)
# Aggregate the data by Join_ID and Category
aggregated_data <- aggregate(sum_Area_SQUAREFEET ~ OID + Category, data = soil_percentage, FUN = sum)
# Aggregate the data by Join_ID and Category
aggregated_data <- aggregate(sum_Area_SQUAREFEET ~ Category, data = soil_percentage, FUN = sum)
# Calculate total area for each OID
total_area <- aggregate(sum_Area_SQUAREFEET ~ OID, data = aggregated_data, FUN = sum)
# Group by Join_ID and Category, then calculate the sum of area for each category
aggregated_data <- soil_percentage %>%
group_by(OID, Category) %>%
summarise(total_area = sum(Area_SQUAREFEET))
# Group by Join_ID and Category, then calculate the sum of area for each category
aggregated_data <- soil_percentage %>%
group_by(Join_ID, Category) %>%
summarise(total_area = sum(Area_SQUAREFEET))
library(dplyr)
# Group by Join_ID and Category, then calculate the sum of area for each category
aggregated_data <- soil_percentage %>%
group_by(Join_ID, Category) %>%
summarise(total_area = sum(Area_SQUAREFEET))
sum_by_category <- soil_percentage %>%
group_by(Join_ID, Category) %>%
summarise(total_area = sum(sum_Area_SQUAREFEET))
View(sum_by_category)
pivoted_data <- spread(sum_by_category, Category, total_area, fill = 0)
# Drop Join_ID duplicates, keeping the first occurrence
pivoted_data <- distinct(pivoted_data, Join_ID, .keep_all = TRUE)
View(pivoted_data)
# Calculate total area for each Join_ID
pivoted_data$total_area <- rowSums(pivoted_data[, -1])
# Convert values to percentages
pivoted_data[, c("A", "B", "C", "B/C", "Unknown")] <- pivoted_data[, c("A", "B", "C", "B/C", "Unknown")] / pivoted_data$total_area * 100
# Remove the total_area column
pivoted_data <- pivoted_data[, -ncol(pivoted_data)]
# View the resulting data frame
head(pivoted_data)
View(pivoted_data)
# Calculate total area for each Join_ID
pivoted_data$total_area <- rowSums(pivoted_data[, -1])
# Convert values to percentages
pivoted_data[, c("A", "B", "C", "B/C", "Unknown")] <- pivoted_data[, c("A", "B", "C", "B/C", "Unknown")] / pivoted_data$total_area * 100
# Remove the total_area column
pivoted_data <- pivoted_data[, -ncol(pivoted_data)]
setwd("D:/Upenn/2024 Spring/MUSA Practicum/Musa_801_Watauga/Data/natural")
View(sum_by_category)
merged_data <- merge(final_parcel, pivoted_data, by.x = "FID", by.y = "Join_ID", all.x = TRUE)
setwd("D:/Upenn/2024 Spring/MUSA Practicum/Musa_801_Watauga/Data/natural")
st_write(merged_data, "natural_model.geojson")
View(merged_data)
pivoted_data[is.na(pivoted_data)] <- 0
merged_data <- merge(final_parcel, pivoted_data, by.x = "FID", by.y = "Join_ID", all.x = TRUE)
st_write(merged_data, "natural_model.geojson")
View(merged_data)
export <- merged_data [c('landslide.nn',
'watershed.nn',
'Dist_Road',
'Dist_Flowline',
'Slope Ave',
'Slope Max',
'Slope Min',
"FID", "A", "B", "B/C","C", "Unknown", "geometry.1")]
export <- merged_data [c('landslide.nn',
'watershed.nn',
'Dist_Road',
'Dist_Flowline',
'Slope_Ave',
'Slope_Max',
'Slope_Min',
"FID", "A", "B", "B/C","C", "Unknown", "geometry.1")]
View(export)
export <- merged_data [c('FID',
'landslide.nn',
'watershed.nn',
'Dist_Road',
'Dist_Flowline',
'Slope_Ave',
'Slope_Max',
'Slope_Min',
"A", "B", "B/C","C", "Unknown", "geometry.1")]
export <- export %>%
rename(Soil_A = A,
Soil_B = B,
Soil_B/C = B/C,
knitr::opts_chunk$set(message = FALSE, warning = FALSE , results='hide')
knitr::opts_chunk$set(echo = TRUE,  message = FALSE, warning = FALSE)
library(tidyverse)
library(sf)
library(RSocrata)
library(viridis)
library(spatstat)
library(raster)
library(spdep)
library(FNN)
library(grid)
library(gridExtra)
library(knitr)
library(kableExtra)
library(tidycensus)
library(classInt)   # for KDE and ML risk class intervals
library(mapview)
# functions
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")
landslide = st_read("https://raw.githubusercontent.com/JarredERandall/Musa_801_Watauga/main/Data/natural/landslide.geojson")
export <- export %>%
rename(Soil_A = A,
Soil_B = B,
Soil_B/C = B/C,
export <- export %>%
rename(Soil_A = 'A',
Soil_B = 'B',
Soil_B/C = 'B/C',
export <- export %>%
rename(Soil_A = A,
Soil_B = B,
Soil_B/C = B/C,
export <- export %>%
rename(Soil_A = A,
Soil_B = B,
Soil_B/C = B/C,
export <- export %>%
rename(Soil_A = A,
Soil_B/C = B/C,
View(export)
selected_data_renamed <- export %>%
rename(Soil_A = A,
Soil_B = B)
# View the resulting dataframe
head(selected_data_renamed)
selected_data_renamed <- export %>%
rename(Soil_A = A,
Soil_B = B,
Soil_B/C = B/C,
selected_data_renamed <- export %>%
rename(Soil_A = A,
Soil_B = B)
selected_data_renamed <- export %>%
rename(Soil_A = A,
Soil_B = B,
Soil_C = C)
selected_data_renamed <- export %>%
rename(Soil_A = A,
Soil_B = B,
Soil_C = C,
Soil_B/C = B/C,
selected_data_renamed <- export %>%
rename(Soil_A = A,
Soil_B = B,
Soil_C = C,
Soil_B/C = 'B/C',
# Rename columns
selected_data_renamed <- export %>%
rename(Soil_A = A,
Soil_B = B,
Soil_C = C,
`Soil_B/C` = `B/C`,  # Using backticks to specify column name with special character
Soil_Unknown = Unknown)
# View the resulting dataframe
head(selected_data_renamed)
View(selected_data_renamed)
st_write(selected_data_renamed, "natural_model.geojson")
View(merged_data)
export <- merged_data [c('FID','PARCELID'
'landslide.nn',
export <- merged_data [c('FID','PARCELID',
'landslide.nn',
'watershed.nn',
'Dist_Road',
'Dist_Flowline',
'Slope_Ave',
'Slope_Max',
'Slope_Min',
"A", "B", "B/C","C", "Unknown", "geometry.1")]
# Rename columns
selected_data_renamed <- export %>%
rename(Soil_A = A,
Soil_B = B,
Soil_C = C,
`Soil_B/C` = `B/C`,  # Using backticks to specify column name with special character
Soil_Unknown = Unknown)
st_write(selected_data_renamed, "natural_model.geojson")
# Assuming your data frame is named df
unique_row_count <- length(unique(final_parcel$GLOBALID))
unique_row_count
# Assuming your data frame is named df
unique_row_count <- length(unique(final_parcel$GlobalID))
unique_row_count
# Assuming your data frame is named df
unique_row_count <- length(unique(final_parcel$PARCELID))
# Assuming your data frame is named df
unique_row_count <- length(unique(final_parcel$GlobalID))
unique_row_count
View(final)
View(parcel_centroids)
