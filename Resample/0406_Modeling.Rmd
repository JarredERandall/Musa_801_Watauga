---
title: "Modeling"
output: html_document
date: "2024-03-25"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,  message = FALSE, warning = FALSE)

library(tidyverse)
library(sf)
library(RSocrata)
library(viridis)
library(spatstat)
library(raster)
library(spdep)
library(FNN)
library(grid)
library(gridExtra)
library(knitr)
library(kableExtra)
library(tidycensus)
library(classInt)   # for KDE and ML risk class intervals
library(mapview)
# functions
root.dir = "https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/"
source("https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/functions.r")
```


# Start Modeling

## 1. Loading Data
```{r}
sample <- st_read('D:/Upenn/2024 Spring/MUSA Practicum/Musa_801_Watauga/Resample/final_modeling.csv')
resample <- st_read("D:/Upenn/2024 Spring/MUSA Practicum/Musa_801_Watauga/Resample/final_modeling_noID.csv")
```
# 2. Start with Logistic Modeling
## 2.1 Building the model
```{r change dataset as numeric data}
sample <- sample %>%
  mutate_if(~ !identical(., sample$GlobalID), as.numeric)

sample <- na.omit(sample)

```

```{r Logistic Modeling}
# Set the seed for reproducibility
set.seed(123)


# Split the data into training and testing sets
train_indices <- sample(1:nrow(sample), 0.8 * nrow(sample))
train_data <- sample[train_indices, ]
test_data <- sample[-train_indices, ]

# Convert permit_22 to a factor variable
train_data$permit_22 <- factor(train_data$permit_22)
test_data$permit_22 <- factor(test_data$permit_22)

# Train the logistic regression model
logistic_model <- glm(permit_22 ~ ., data = train_data%>%dplyr::select(-GlobalID, -n_permit_22), family="binomial" (link="logit"))

# Print summary of the model
summary(logistic_model)
```
```{r p_value of all features}
# Extract coefficient estimates and p-values
coefficients <- summary(logistic_model)$coefficients

# Remove intercept
coefficients <- coefficients[-1, ]

# Plot p-values with rotated labels and reduced font size
barplot(coefficients[, 4], names.arg = rownames(coefficients),
        ylim = c(0, 1), ylab = "p-value", col = "#559a90ff",
        main = "P-values of Logistic Regression Coefficients",
        las = 2, cex.axis = 0.8, cex.lab = 0.8)  # Adjust font size

# Add horizontal line for p = 0.05
abline(h = 0.05, col = "#e2a334ff", lty = 2)  # Add horizontal line at y = 0.05


```
## 2.2 Making Predictions & Researched the metrics

```{r make predictions}
test_predictions <- predict(logistic_model, newdata = test_data, type = "response")

# Convert probabilities to binary predictions (0 or 1) using a threshold of 0.5
binary_predictions <- ifelse(test_predictions >= 0.5, 1, 0)

# Calculate accuracyd
accuracy <- mean(binary_predictions == test_data$permit_22)
cat("Accuracy of the logistic regression model on the test data:", accuracy)
```
```{r Sensitivity}
# Calculate confusion matrix
conf_matrix <- table(test_data$permit_22, binary_predictions)

# Calculate precision, recall, and F1 score
precision <- ifelse(sum(conf_matrix[, 2]) == 0, 0, conf_matrix[2, 2] / sum(conf_matrix[, 2]))
recall <- ifelse(sum(conf_matrix[2, ]) == 0, 0, conf_matrix[2, 2] / sum(conf_matrix[2, ]))
f1_score <- ifelse(precision + recall == 0, 0, 2 * precision * recall / (precision + recall))

# Print F1 score
cat("F1 score of the logistic regression model on the test data:", f1_score)
```


```{r testProb}
testProbs <- data.frame(Outcome = as.factor(test_data$permit_22),
                        Probs = test_predictions)
head(testProbs)
```

```{r}
testProbs <- 
  testProbs %>%
  mutate(predOutcome  = as.factor(ifelse(testProbs$Probs > 0.5 , 1, 0)))

head(testProbs)
```
```{r}
caret::confusionMatrix(testProbs$predOutcome, testProbs$Outcome, 
                       positive = "1")
```
```{r ROC Curve}
library(plotROC)

ggplot(testProbs, aes(d = as.numeric(testProbs$Outcome), m = Probs)) +
  geom_roc(n.cuts = 50, labels = FALSE, colour = "#FE9900") +
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1.5, color = 'grey') +
  labs(title = "ROC Curve - churnModel")
```
```{r AUC}
library(pROC)

# Assuming the logistic regression model is already fitted as 'logistic_model'

# Predict probabilities on the test data
predicted_probabilities <- predict(logistic_model, newdata = test_data, type = "response")

# Calculate ROC curve
roc_curve <- roc(test_data$permit_22, predicted_probabilities)

# Calculate AUC
auc_value <- auc(roc_curve)

# Print AUC value
print(auc_value)
```

```{r Model Distribution}
# Predict probabilities on the test data
predicted_probabilities <- predict(logistic_model, newdata = test_data, type = "response")

# Create dataframe for observed and predicted probabilities
roc_curve <- data.frame(obs = as.numeric(test_data$permit_22),
                        pred = predicted_probabilities)

# Plot density curves
ggplot(roc_curve, aes(x = pred, fill = as.factor(obs))) + 
  geom_density() +
  facet_grid(obs ~ .) + 
  xlab("Probability") +
  ylab("Frequency") +
  geom_vline(xintercept = 0.5) +
  scale_fill_manual(values = c("#559a90ff", "#e2a334ff"),
                    labels = c("Not Developed", "Developed"),
                    name = "") +
  theme_minimal()

```

## 2.3 Try to get the best threshold
```{r}
cost_benefit_table <-
   testProbs %>%
      count(predOutcome, Outcome) %>%
      summarize(True_Negative = sum(n[predOutcome==0 & Outcome==0]),
                True_Positive = sum(n[predOutcome==1 & Outcome==1]),
                False_Negative = sum(n[predOutcome==0 & Outcome==1]),
                False_Positive = sum(n[predOutcome==1 & Outcome==0])) %>%
       gather(Variable, Count) %>%
    bind_cols(data.frame(Description = c(
              "We predicted no churn and did not send a mailer",
              "We predicted churn and sent the mailer",
              "We predicted no churn and the customer churned",
              "We predicted churn and the customer did not churn")))
```

The logistic regression does not work well for the dataset since there is no true positive for it, which is bad for it.


Then try to improve the threshold
```{r}
whichThreshold <- 
  iterateThresholds(
     data=testProbs, observedClass = Outcome, predictedProbs = Probs)

whichThreshold[1:5,]
```


```{r}
ggplot(whichThreshold, aes(x = Rate_TP, y = Accuracy)) +
  geom_point(color = "#559a90ff") +  # Set point color to red
  geom_line(color = "#559a90ff") +    # Set line color to red
  labs(x = "Rate_TP", y = "Accuracy", title = "Relationship between RateTP and Accuracy")

```
Try when Rate_TP = 0.5, threshold = 0.12

```{r}
test_predictions <- predict(logistic_model, newdata = test_data, type = "response")

# Convert probabilities to binary predictions (0 or 1) using a threshold of 0.5
binary_predictions <- ifelse(test_predictions >= 0.12, 1, 0)

# Calculate accuracy
accuracy <- mean(binary_predictions == test_data$permit_22)
cat("Accuracy of the logistic regression model on the test data:", accuracy)
```

Risk estimate; what will happen if you follow a predict that is wrong.
- labor intensive to investigate a parcel: FP
- How many parcels? Maybe TOP...? 

We know that we could do ... but it might be terrible TP when
FP will ....

Framework for ... do not have hard threshold 


```{r}
conf_matrix <- table(test_data$permit_22, binary_predictions)

# Get the true positive (TP) count
TP <- conf_matrix[2, 2]

# Get the total number of actual positive cases
actual_positive <- sum(test_data$permit_22 == 1)

# Calculate the true positive rate (TPR) or sensitivity
TP_rate <- TP / actual_positive

# Print the true positive rate
cat("True Positive Rate (TPR) of the logistic regression model:", TP_rate)
```


# 3.Random forest model
```{r Rf model building}
library(randomForest)

# Train the Random Forest model with tuned hyperparameters
rf_model <- randomForest(permit_22 ~ ., data = train_data%>%dplyr::select(-GlobalID, -n_permit_22),
                         ntree = 500, mtry = sqrt(ncol(train_data)))

# Make predictions on the test data
rf_predictions_prob <- predict(rf_model, newdata = test_data, type = "prob")
```

## 3.2 Some metrics
```{r Accuracy of the RF model}
# Convert predicted probabilities to class predictions
rf_predictions <- ifelse(rf_predictions_prob[, "1"] >= 0.5, 1, 0)

# Calculate accuracy
accuracy <- mean(rf_predictions == test_data$permit_22)

# Print the accuracy
cat("Accuracy of the Random Forest model on the test data:", accuracy)

```


```{r f1 score}
# Calculate confusion matrix
conf_matrix <- table(test_data$permit_22, rf_predictions )

# Calculate precision, recall, and F1 score
precision <- ifelse(sum(conf_matrix[, 2]) == 0, 0, conf_matrix[2, 2] / sum(conf_matrix[, 2]))
recall <- ifelse(sum(conf_matrix[2, ]) == 0, 0, conf_matrix[2, 2] / sum(conf_matrix[2, ]))
f1_score <- ifelse(precision + recall == 0, 0, 2 * precision * recall / (precision + recall))

# Print F1 score
cat("F1 score of the Random Forest model on the test data:", f1_score)

```
This model seems to be overfit.

```{r TPR}
# Get the true positive (TP) count
TP <- conf_matrix[2, 2]

# Get the total number of actual positive cases
actual_positive <- sum(test_data$permit_22 == 1)

# Calculate the true positive rate (TPR) or sensitivity
TPR <- TP / actual_positive

# Print the true positive rate
cat("True Positive Rate (TPR) of the Random Forest model:", TPR)
```

```{r get the metrics table}
testProbs <- data.frame(Outcome = as.factor(test_data$permit_22),
                        Probs = rf_predictions)
testProbs <- 
  testProbs %>%
  mutate(predOutcome  = as.factor(ifelse(testProbs$Probs > 0.5 , 1, 0)))

head(testProbs)
```

```{r}
caret::confusionMatrix(testProbs$predOutcome, testProbs$Outcome, 
                       positive = "1")
```


```{r ROC}
ggplot(testProbs, aes(d = as.numeric(testProbs$Outcome), m = Probs)) +
  geom_roc(n.cuts = 50, labels = FALSE, colour = "#FE9900") +
  style_roc(theme = theme_grey) +
  geom_abline(slope = 1, intercept = 0, size = 1.5, color = 'grey') +
  labs(title = "ROC Curve - churnModel")

```
```{r}
# Predict probabilities on the test data
rf_predictions <- predict(rf_model, newdata = test_data, type = "prob")
rf_predictions <- ifelse(rf_predictions_prob[, "1"] >= 0.5, 1, 0)

# Create dataframe for observed and predicted probabilities
testProbs <- data.frame(obs = as.numeric(test_data$permit_22),
                        pred = rf_predictions)

# Plot density curves
ggplot(testProbs, aes(x = pred, fill = as.factor(obs))) + 
  geom_density() +
  facet_grid(obs ~ .) + 
  xlab("Probability") +
  ylab("Frequency") +
  geom_vline(xintercept = 0.5) +
  scale_fill_manual(values = c("#559a90ff", "#e2a334ff"),
                    labels = c("Not Developed", "Developed"),
                    name = "") +
  theme_minimal()

```
# 4.Pisson regression

```{r Pisson Regression}
set.seed(123)

# Split the data into training and testing sets
train_indices <- sample(1:nrow(sample), 0.8 * nrow(sample))
train_data <- sample[train_indices, ]
test_data <- sample[-train_indices, ]

# Convert permit_22 to a factor variable
train_data$permit_22 <- factor(train_data$permit_22)
test_data$permit_22 <- factor(test_data$permit_22)


poisson_model <- glm(n_permit_22 ~ ., data = train_data%>%dplyr::select(-GlobalID, -permit_22), family = "poisson")

# Summary of the model
summary(poisson_model)
```

```{r model MAE}
# Assuming you have a test dataset called test_data

# Making predictions on test data
predictions <- predict(poisson_model, newdata = test_data, type = "response")

# Removing missing values from both predictions and actual values
predictions <- predictions[!is.na(predictions)]
actual_values <- test_data$n_permit_22[!is.na(predictions)]

# Calculating mean absolute error
mae <- mean(abs(predictions - actual_values))

# Printing mean absolute error
print(paste("Mean Absolute Error:", mae))

```

```{r}
# Define the number of folds for cross-validation
k <- 5

# Initialize an empty dataframe to store results
results_df <- data.frame(Fold = numeric(),
                         MAE = numeric())

# Perform cross-validation
for (i in 1:k) {
  # Create indices for cross-validation folds
  set.seed(123)  # Set seed for reproducibility
  folds <- sample(1:k, nrow(sample), replace = TRUE)
  
  # Split the data into training and test sets
  test_indices <- which(folds == i)
  train_data <- sample[-test_indices, ]
  test_data <- sample[test_indices, ]
  
  # Fit the Poisson regression model on the training data
  poisson_model <- glm(n_permit_22 ~ ., data = train_data %>% dplyr::select(-GlobalID, -permit_22), family = "poisson")
  
  # Make predictions on the test data
  predictions <- predict(poisson_model, newdata = test_data, type = "response")
  
  # Calculate mean absolute error for this fold
  fold_mae <- mean(abs(predictions - test_data$n_permit_22))
  
  # Store results in the dataframe
  results_df <- rbind(results_df, data.frame(Fold = i, MAE = fold_mae))
}

# Print the dataframe
print(results_df)

mean_mae <- mean(mae_values)
print(paste("Overall Mean Absolute Error:", mean_mae))
```

```{r}
# Calculate the overall mean absolute error
overall_mean <- mean(results_df$MAE)

# Plot the results stored in results_df
barplot(results_df$MAE, 
        names.arg = results_df$Fold,
        main = "Mean Absolute Errors for Each Fold",
        xlab = "Fold",
        ylab = "Mean Absolute Error",
        col = "#559a90ff",
        border = "black",
        ylim = c(0, 0.4))

# Add mean line
abline(h = overall_mean, col = "black", lty = 2)
text(x = 0.5, y = overall_mean, labels = round(overall_mean, 4), pos = 4, col = "black")

# Calculate the position of the legend
par(xpd=TRUE) # allow plotting outside the plot area
legend("topright", 
       legend = c("Mean Absolute Error", "Overall Mean Absolute Error"), 
       col = c("#559a90ff", "red"), 
       lty = c(1, 2), 
       bty = "n")

# Reset the plotting area
par(xpd=FALSE)
```